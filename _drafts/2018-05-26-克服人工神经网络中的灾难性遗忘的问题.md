---
layout: post
title: 克服人工神经网络中的灾难性遗忘的问题
tags: [paper, blog]
---

> 这是[原文地址](http://www.pnas.org/content/early/2017/03/13/1611835114)。这篇文章是
> deepmind的研究成果。暂时不对内容进行评论。
>
> 另有一篇评论文章： [http://www.inference.vc/comment-on-overcoming-catastrophic-forgetting-in-nns-are-multiple-penalties-needed-2/](http://www.inference.vc/comment-on-overcoming-catastrophic-forgetting-in-nns-are-multiple-penalties-needed-2/)


### 摘要：
The ability to learn tasks in a sequential fashion is crucial to the development of
artificial intelligence. Neural networks are not, in general, capable of this and it
has been widely thought that catastrophic forgetting is an inevitable feature of
connectionist models. We show that it is possible to overcome this limitation and
train networks that can maintain expertise on tasks which they have not experienced
for a long time. Our approach remembers old tasks by selectively slowing down
learning on the weights important for those tasks. We demonstrate our approach is
scalable and effective by solving a set of classification tasks based on the MNIST
hand written digit dataset and by learning several Atari 2600 games sequentially.

按照顺序来学习的方式对于今天的人工智能的发展非常重要。但是，通常人工神经网络被认为不具备这项能力，因为其灾难性
遗忘的特性被人们认为是在链接性模型中所不可避免的。在本文中，我们展示了一种克服这种窘境的可能，也即是人工神经网络能够
在即便是很久以前经历的任务上展现出专家水平。
