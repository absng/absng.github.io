---
layout: post
title: 使用增强学习方法解魔方问题
tags: [blog, python]
---

### 项目最近还在慢慢的进行中

假如说直接将魔方旋转5次以上，允许RL使用MC方法直接暴力破解然后学习，这样子见效太慢。

我在这个程序内使用的*循序渐进*的办法：

将模仿旋转1次，然后让RL学习如何解一次旋转的魔方。然后旋转两次，RL会学习解第二次旋转的问题，依次渐进，将一个复杂的见效慢的问题逐步解决

使用Q-table的问题：

Qtable的好处也是见效比较快，实现起来比较简单。

但是问题是，当状态数量变得逐渐多的时候，例如魔方旋转5次。Qtable jsonized的文件大小达到了500MB以上。

所以下一步就要使用神经网络来代替Q-table

决定采用A2C模式

使用Q-table生成两份训练数据，一份数据用来训练Critic，数据包含状态（X），和各动作的Value（Y）
一份数据用来训练Actor, 数据包含状态(X), 和从各动作的value中提取出来的执行的动作(Y one-hoted)

这样训练出来的Actor, Critic就是两个预训练的网络，应该训练完成之后就可以处理五次旋转的魔方

#### *A glance*
[http://nbviewer.jupyter.org/github/cinqs/rubikcubesolverbyrl/blob/master/Untitled.ipynb](http://nbviewer.jupyter.org/github/cinqs/rubikcubesolverbyrl/blob/master/Untitled.ipynb)

#### *of course the code*
[https://github.com/cinqs/rubikcubesolverbyrl](https://github.com/cinqs/rubikcubesolverbyrl)

#### *the dumped Q table(with format tricked)*
[https://github.com/cinqs/rubikcubesolverbyrl/blob/master/Q.json](https://github.com/cinqs/rubikcubesolverbyrl/blob/master/Q.json)

> Thanks for the project of [Adrian Liaw](https://github.com/adrianliaw/PyCuber)
